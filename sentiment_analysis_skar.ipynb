{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_skar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9hQzVRmLT0pNaXby/cBNg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivam-karthik/sentiment_analysis/blob/main/sentiment_analysis_skar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wqeCaRc6MmXz"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import nltk, re, string\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pickle  #load and store modal and data from the disk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creation of function for preprocessing of the tweets(the data)\n",
        " \n",
        "def process_tweet(tweet):\n",
        "   #using porterstemmer algorithm for cleaning words\n",
        "    stemmer = nltk.PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and\n",
        "                word not in string.punctuation):\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "qO5hahg23jtX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating function to count frequency of occurance of words in  a tweet\n",
        "#this function will return a dictionary of keyname equivalemt of a tuple and a-\n",
        "# value of either 0 or 1\n",
        "def build_freqs(tweets, ys):\n",
        "     \"\"\"Build frequencies \n",
        "     i/p \n",
        "        tweets : list of tweet\n",
        "        ys : m*1 array with sentiment label(0 or 1) of each tweet\n",
        "\n",
        "        o/p- freq- a dictionary mapping each (word,sentiment) pair to its \n",
        "        frequency\n",
        "     \"\"\"\n",
        "\n",
        "    #removing axes of length one from ys using squeeze() to make it zero dimensional array.\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "\n",
        "     yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "   \n",
        "\n",
        "    #creating a zipped list of yslist, tweets which have tuple elements ('y', tweet)\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "     freqs = {}\n",
        "     for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "     return freqs\n",
        "      "
      ],
      "metadata": {
        "id": "YWKW-iX0FbbH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking how above code works\n",
        "\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "ys = [1, 0 ,0, 0, 0]\n",
        "res= build_freqs(tweets, ys)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "LXWXT_PS7_07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba44eb0-8aa5-4bb2-f999-756211256b9a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HtFUfmvUsxr",
        "outputId": "736d7cf8-c49e-4fd9-e06c-ffe95f10235a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting the dataset\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n"
      ],
      "metadata": {
        "id": "dUPzVPtOVhuD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting tehe dataset into training and test set\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n"
      ],
      "metadata": {
        "id": "OVlhmynAWLTr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n"
      ],
      "metadata": {
        "id": "Uvcao-PlXLFT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine positive and negative labels\n",
        "#we are building our y -target varaible here\n",
        "#numpy.append appends both trains pos and train neg along one list or axis\n",
        "#we have passed dimensions of array in numpy.append##\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "lnNxHZGOXbOQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create frequency dictionary\n",
        "\n",
        "freqs = build_freqs(train_x, train_y)"
      ],
      "metadata": {
        "id": "iv7lUe-GpRQ5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check out the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wULSUP5gprbJ",
        "outputId": "3a0c400b-0e57-4e98-afc1-d992a012f5c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test the function below\n",
        "\n",
        "print(\"this is an example of a positive tweet: \\n\", train_x[22])\n",
        "print(\"\\nthis is an example of the processed version of the tweet\", process_tweet(train_x[22]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vam4Dm6gqMd6",
        "outputId": "e70df60d-e261-45ad-d89e-37375a8a5a2c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is an example of a positive tweet: \n",
            " @gculloty87 Yeah I suppose she was lol! Chat in a bit just off out x :))\n",
            "\n",
            "this is an example of the processed version of the tweet ['yeah', 'suppos', 'lol', 'chat', 'bit', 'x', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OEGJnI9Cj0Ku"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fX-kl3UGhtVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will build a logiatic regression model from scratch\n",
        "\n",
        "#lets create a sigmoid function\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    \"\"\"\n",
        "    zz = np.negative(z)\n",
        "    h = 1 / (1 + np.exp(zz))\n",
        "    return h"
      ],
      "metadata": {
        "id": "wJY-eaP1rR8s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets define the cost function and Gradientfor our lofistic model\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y:corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta : weight vector of dimension (n+1,1)\n",
        "        alpha: learning vector\n",
        "        num_iters : number of iterations you want to train your model for\n",
        "        m: the number of rows in matrix x\n",
        "    Output:\n",
        "         J: the final cost\n",
        "         theta: final weight vector\n",
        "        \n",
        "    \"\"\"  \n",
        "    #cost function\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = x.shape[0]\n",
        "    for i in range(0, num_iters):\n",
        "        z = np.dot(x, theta)\n",
        "        h = sigmoid(z)\n",
        "        # calculate the cost function\n",
        "        cost = -1. / m * (np.dot(y.transpose(), np.log(h)) + np.dot((1 - y).transpose(), np.log(1 - h)))\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha / m) * np.dot(x.transpose(), (h - y))\n",
        "\n",
        "    cost = float(cost)\n",
        "    return cost, theta\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "R1JqsixTscYR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extraction\n",
        "\n",
        "def extract_features(tweet, freqs):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "       tweeet: a list of words for one tweet\n",
        "       freqs : a dictionary corresponding to the frequencies of each tuple(word,label)\n",
        "  output:\n",
        "       x: A feature vector of dimension (1,3)\n",
        "  \"\"\"\n",
        "\n",
        "  word_l = process_tweet(tweet)\n",
        "  x= np.zeros((1,3))\n",
        "\n",
        "  #bias term is set to 1\n",
        "  x[0,0] = 1\n",
        "\n",
        "  for word in word_l:\n",
        "    #increment the word count for the positive label 1\n",
        "    #using dictionary.get(keyname, value to be returned if key is not found)\n",
        "    \n",
        "    x[0,1] += freqs.get((word, 1.0),0)\n",
        "    #increment the word count for the negative label 0\n",
        "    x[0,2] += freqs.get((word, 0.0), 0)\n",
        "\n",
        "  assert (x.shape == (1,3))\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "1sc2lTwbXWNA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #just a way to go about things happening here or above#\n",
        "# word_l = process_tweet(train_x[22])\n",
        "# for word in word_1:\n",
        "#   print(freqs.get((word, 0.0), 0))"
      ],
      "metadata": {
        "id": "y9MSA4d_j-H0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test on training data\n",
        "tmp1 = extract_features(train_x[22], freqs)\n",
        "print(tmp1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCSrfX5reTil",
        "outputId": "cb7b69c4-c568-4102-dc75-681f0067ddca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.000e+00 3.006e+03 1.240e+02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#above three numbers are our feautre vector\n",
        "#hypothesis h(x) = b1+ b2*x1 + b3*x2 , b2,b3 will be calculated by cost and gradient \n",
        "#as bias = b1 =1 also x1 and x2 are positive and negative feature set\n"
      ],
      "metadata": {
        "id": "ETNrJVTNeqau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Your Model\n",
        "\n",
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :] = extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "# these values are predefined (Andrew NG)\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n"
      ],
      "metadata": {
        "id": "yL0CHtHQxYMJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output:\n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    \"\"\"\n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet, freqs)\n",
        "    y_pred = sigmoid(np.dot(x, theta))\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "JdOb6PWk6l8j"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "\n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        if y_pred > 0.5:\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            y_hat.append(0)\n",
        "\n",
        "    accuracy = (y_hat == np.squeeze(test_y)).sum() / len(test_x)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "BoH3kyP88DB7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"LR model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1E0CBVTBEZU",
        "outputId": "5770d193-b20e-471e-ad3b-bf6c78e8227c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR model's accuracy = 0.9950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict your tweet\n",
        "\n",
        "def pre(sentence):\n",
        "  yhat = predict_tweet(sentence, freqs, theta)\n",
        "  if yhat> 0.5:\n",
        "     return 'positive sentiment'\n",
        "  elif yhat == 0:\n",
        "    return 'neutral sentiment'\n",
        "  else: \n",
        "    return 'negative sentiment'\n"
      ],
      "metadata": {
        "id": "w_K5EHGiCKYI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweet = 'find your fire'\n",
        "\n",
        "res = pre(test_tweet)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oexbLw7DEDx8",
        "outputId": "67c45a1b-13f3-42c4-f123-07d602ccd8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kGxuz92TKsLu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}